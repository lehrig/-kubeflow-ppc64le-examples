{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Genre Classification\n",
    "\n",
    "Classify the musical genre of a given audio track - from data to deployed model. Adapted from the work of Huang, Serafini, and Pugh [1].\n",
    "\n",
    "## Authors\n",
    "- Sebastian Lehrig <sebastian.lehrig1@ibm.com>\n",
    "- Marvin Giessing <MARVING@de.ibm.com>\n",
    "\n",
    "## License\n",
    "Apache-2.0 License\n",
    "\n",
    "## References\n",
    "[1] original paper: http://cs229.stanford.edu/proj2018/report/21.pdf\n",
    "\n",
    "[2] code: https://github.com/derekahuang/Music-Classification\n",
    "\n",
    "[3] Preprocessed Data: https://drive.google.com/file/d/12mCgkvbmissLh2Vop0bp_t98G8QCaV1E/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.) Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "import json\n",
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "from kubernetes.client.models import V1Volume, V1PersistentVolumeClaimVolumeSource\n",
    "import librosa as lb\n",
    "from librosa import display\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from pydoc import importfile\n",
    "import pylab\n",
    "import requests\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment-specific configurations\n",
    "# - Activate train-bursting by setting a CLUSTER_CONFIGURATION_SECRET containing the remote cluster configuration\n",
    "# - Activate distributed training by setting NUMBER_OF_WORKERS > 1; TRAINING_GPUS hold per worker\n",
    "#\n",
    "# %env CLUSTER_CONFIGURATION_SECRET remote-power-cluster\n",
    "# %env CLUSTER_CONFIGURATION_SECRET remote-x86-cluster\n",
    "# %env CLUSTER_CONFIGURATION_SECRET remote-x86-telekom-cluster\n",
    "# %env TRAINING_GPUS 0\n",
    "# %env NUMBER_OF_WORKERS 1\n",
    "# %env TRAINING_NODE_SELECTOR nvidia.com/gpu.product: \"Tesla-V100-SXM2-32GB\"\n",
    "# %env TRAINING_NODE_SELECTOR kubernetes.io/hostname: node2\n",
    "# %env TRAINING_NODE_SELECTOR worker_type: baremetal_worker\n",
    "#\n",
    "# Reset:\n",
    "# del os.environ['CLUSTER_CONFIGURATION_SECRET']\n",
    "# del os.environ['TRAINING_GPUS']\n",
    "# del os.environ['TRAINING_NODE_SELECTOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\"\n",
    "COMPONENT_CATALOG_GIT = \"https://github.com/lehrig/kubeflow-ppc64le-components.git\"\n",
    "COMPONENT_CATALOG_RELEASE = \"main\"\n",
    "\n",
    "NUMBER_OF_WORKER = os.getenv(\"NUMBER_OF_WORKERS\", default=\"1\")\n",
    "\n",
    "ARGUMENTS = {\n",
    "    \"blackboard\": \"artefacts\",\n",
    "    \"dataset_url\": \"Lehrig/GTZAN-Collection\",\n",
    "    \"dataset_configuration\": \"mel_spectrograms\",\n",
    "    \"dataset_label_columns\": [\"genre\"],\n",
    "    \"model_name\": \"music-classification\",\n",
    "    \"cluster_configuration_secret\": os.getenv(\n",
    "        \"CLUSTER_CONFIGURATION_SECRET\", default=\"\"\n",
    "    ),\n",
    "    \"training_gpus\": os.getenv(\"TRAINING_GPUS\", default=\"0\"),\n",
    "    \"number_of_workers\": NUMBER_OF_WORKER,\n",
    "    \"distribution_type\": \"Job\" if int(NUMBER_OF_WORKER) <= 1 else \"MPI\",\n",
    "    \"training_node_selector\": os.getenv(\"TRAINING_NODE_SELECTOR\", default=\"\"),\n",
    "}\n",
    "MODEL_NAME = ARGUMENTS[\"model_name\"]\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "    NAMESPACE = f.read()\n",
    "\n",
    "ARGUMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Load catalog with reusable Kubeflow components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --branch $COMPONENT_CATALOG_RELEASE $COMPONENT_CATALOG_GIT $COMPONENT_CATALOG_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = importfile(f\"{COMPONENT_CATALOG_FOLDER}/catalog.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Create custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Component: Preprocess data (one hot encoding etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(\n",
    "    dataset_dir: InputPath(str),\n",
    "    train_dataset_dir: OutputPath(str),\n",
    "    validation_dataset_dir: OutputPath(str),\n",
    "    test_dataset_dir: OutputPath(str),\n",
    "    batch_size: int = 200,\n",
    "):\n",
    "    \"\"\"Split data into train/dev/test data. Saves result into `prep_dataset_dir`.\"\"\"\n",
    "\n",
    "    from datasets import load_from_disk\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from transformers import DefaultDataCollator\n",
    "\n",
    "    print(f\"Loading input dataset from {dataset_dir}...\")\n",
    "    dataset = load_from_disk(dataset_dir)\n",
    "\n",
    "    # Preprocess\n",
    "    num_classes = dataset[\"train\"].features[\"genre\"].num_classes\n",
    "    one_hot_matrix = np.eye(num_classes)\n",
    "\n",
    "    def process(examples):\n",
    "        examples[\"genre\"] = [one_hot_matrix[genre] for genre in examples[\"genre\"]]\n",
    "        return examples\n",
    "\n",
    "    prep_dataset = dataset.map(\n",
    "        process, batched=True, batch_size=batch_size, num_proc=2, keep_in_memory=True\n",
    "    )\n",
    "\n",
    "    def save_as_tfdataset(\n",
    "        dataset, columns, label_columns, data_collator, directory, shuffle\n",
    "    ):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        tf_dataset = dataset.to_tf_dataset(\n",
    "            columns=columns,\n",
    "            label_cols=label_columns,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving pre-processed dataset to '{directory}'...\")\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        tf.data.Dataset.save(tf_dataset, directory)\n",
    "\n",
    "        print(f\"Pre-processed dataset saved. Contents of '{directory}':\")\n",
    "        print(os.listdir(directory))\n",
    "\n",
    "    # prep_dataset = prep_dataset.with_format(\"numpy\")\n",
    "    data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "    columns = [\"mel_spectrogram\"]\n",
    "    label_columns = [\"genre\"]\n",
    "    save_as_tfdataset(\n",
    "        prep_dataset[\"train\"],\n",
    "        columns,\n",
    "        label_columns,\n",
    "        data_collator,\n",
    "        train_dataset_dir,\n",
    "        True,\n",
    "    )\n",
    "    save_as_tfdataset(\n",
    "        prep_dataset[\"validation\"],\n",
    "        columns,\n",
    "        label_columns,\n",
    "        data_collator,\n",
    "        validation_dataset_dir,\n",
    "        False,\n",
    "    )\n",
    "    save_as_tfdataset(\n",
    "        prep_dataset[\"test\"],\n",
    "        columns,\n",
    "        label_columns,\n",
    "        data_collator,\n",
    "        test_dataset_dir,\n",
    "        False,\n",
    "    )\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "\n",
    "preprocess_dataset_comp = kfp.components.create_component_from_func(\n",
    "    func=preprocess_dataset, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Specification: Train the model (used as parameter to train component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_dataset_dir: InputPath(str),\n",
    "    validation_dataset_dir: InputPath(str),\n",
    "    model_dir: OutputPath(str),\n",
    "    epochs: int = 100,\n",
    "    batch_size: int = 200,\n",
    "):\n",
    "    \"\"\"Trains CNN model. Once trained, the model is persisted to `model_dir`.\"\"\"\n",
    "\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.callbacks import (\n",
    "        EarlyStopping,\n",
    "        ModelCheckpoint,\n",
    "        ReduceLROnPlateau,\n",
    "        TensorBoard,\n",
    "    )\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import (\n",
    "        BatchNormalization,\n",
    "        Conv2D,\n",
    "        Dense,\n",
    "        Dropout,\n",
    "        Flatten,\n",
    "        MaxPooling2D,\n",
    "    )\n",
    "    from tensorflow.keras import regularizers\n",
    "    import time\n",
    "\n",
    "    def load_datasets():\n",
    "        train_dataset = tf.data.Dataset.load(train_dataset_dir)\n",
    "        validation_dataset = tf.data.Dataset.load(validation_dataset_dir)\n",
    "        return (train_dataset, validation_dataset)\n",
    "\n",
    "    def build_model():\n",
    "        model = Sequential()\n",
    "\n",
    "        # Feature Learning Layers\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                64,\n",
    "                kernel_size=(4, 4),\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(0.04),\n",
    "                input_shape=(64, 173, 1),\n",
    "            )\n",
    "        )\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                64, (3, 5), activation=\"relu\", kernel_regularizer=regularizers.l2(0.04)\n",
    "            )\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                64, (2, 2), activation=\"relu\", kernel_regularizer=regularizers.l2(0.04)\n",
    "            )\n",
    "        )\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # Classification Layers\n",
    "        model.add(Flatten())\n",
    "        model.add(\n",
    "            Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.04))\n",
    "        )\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(\n",
    "            Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(0.04))\n",
    "        )\n",
    "        model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "        return model\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset, validation_dataset = load_datasets()\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    model = build_model()\n",
    "    print(model.summary())\n",
    "\n",
    "    print(\"Compiling model...\")\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"categorical_accuracy\"],\n",
    "    )\n",
    "\n",
    "    print(\"Initializing training callbacks...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=20, verbose=0, mode=\"min\"),\n",
    "        ModelCheckpoint(\n",
    "            f\"{model_dir}/best_model.keras\",\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.1,\n",
    "            patience=7,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=os.environ[\"TENSORBOARD_S3_ADDRESS\"],\n",
    "            histogram_freq=1,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    print(\"Starting model training...\")\n",
    "    start = time.time()\n",
    "    hist = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    print(\"\\n\\nTraining took \", time.time() - start, \"seconds\")\n",
    "\n",
    "    print(\"Model train history:\")\n",
    "    print(hist.history)\n",
    "\n",
    "    print(f\"Saving model to: {model_dir}\")\n",
    "    model.save(model_dir)\n",
    "    print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "    print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Training: Un-collapse below cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_distributed_model(\n",
    "    train_dataset_dir: InputPath(str),\n",
    "    validation_dataset_dir: InputPath(str),\n",
    "    model_dir: OutputPath(str),\n",
    "    epochs: int = 100,\n",
    "    batch_size: int = 200,\n",
    "):\n",
    "    \"\"\"Trains CNN model. Once trained, the model is persisted to `model_dir`.\"\"\"\n",
    "\n",
    "    import horovod.tensorflow.keras as hvd\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.callbacks import (\n",
    "        EarlyStopping,\n",
    "        ModelCheckpoint,\n",
    "        ReduceLROnPlateau,\n",
    "        TensorBoard,\n",
    "    )\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import (\n",
    "        BatchNormalization,\n",
    "        Conv2D,\n",
    "        Dense,\n",
    "        Dropout,\n",
    "        Flatten,\n",
    "        MaxPooling2D,\n",
    "    )\n",
    "    from tensorflow.keras import regularizers\n",
    "    import tensorflow_datasets as tfds\n",
    "    import time\n",
    "\n",
    "    def load_datasets():\n",
    "        train_dataset = tf.data.Dataset.load(train_dataset_dir)\n",
    "        validation_dataset = tf.data.Dataset.load(validation_dataset_dir)\n",
    "        return (train_dataset, validation_dataset)\n",
    "\n",
    "    def build_model():\n",
    "        model = Sequential()\n",
    "\n",
    "        # Feature Learning Layers\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                64,\n",
    "                kernel_size=(4, 4),\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(0.04),\n",
    "                input_shape=(64, 173, 1),\n",
    "            )\n",
    "        )\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                64, (3, 5), activation=\"relu\", kernel_regularizer=regularizers.l2(0.04)\n",
    "            )\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                64, (2, 2), activation=\"relu\", kernel_regularizer=regularizers.l2(0.04)\n",
    "            )\n",
    "        )\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # Classification Layers\n",
    "        model.add(Flatten())\n",
    "        model.add(\n",
    "            Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.04))\n",
    "        )\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(\n",
    "            Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(0.04))\n",
    "        )\n",
    "        model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "        return model\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    print(\"Initializing Horovod/MPI for distributed training...\")\n",
    "    hvd.init()\n",
    "\n",
    "    # Pin GPU to be used to process local rank (one GPU per process)\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], \"GPU\")\n",
    "\n",
    "    # Prepare distributed training with GPU support\n",
    "    os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "    tfds.disable_progress_bar()\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "    # see https://horovod.readthedocs.io/en/stable/api.html\n",
    "    print(\"==============================================\")\n",
    "    print(f\"hvd.rank(): {str(hvd.rank())}\")\n",
    "    print(f\"hvd.local_rank(): {str(hvd.local_rank())}\")\n",
    "    print(f\"hvd.size(): {str(hvd.size())}\")\n",
    "    print(f\"hvd.local_size(): {str(hvd.local_size())}\")\n",
    "    print(\"gpus:\")\n",
    "    print(gpus)\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset, validation_dataset = load_datasets()\n",
    "\n",
    "    print(\"Making traininig dataset ready for distributed training...\")\n",
    "    # Best shuffling needs a buffer with size equal to the size of the\n",
    "    # dataset. Approximate values should be fine here.\n",
    "    dataset_elements = 1000  # hard to determine dynamically in TFDataset\n",
    "    approx_shard_train_size = dataset_elements // hvd.size() + 1\n",
    "\n",
    "    # References:\n",
    "    # - shard: https://github.com/horovod/horovod/issues/2623#issuecomment-768435610\n",
    "    # - cache & prefetch: https://stackoverflow.com/questions/59228816/what-do-the-tensorflow-datasets-functions-cache-and-prefetch-do\n",
    "    # - shuffle: https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling\n",
    "    distributed_train_dataset = (\n",
    "        train_dataset.unbatch()  # Batch after sharding\n",
    "        .shard(num_shards=hvd.size(), index=hvd.rank())  # 1 shard per worker\n",
    "        .cache()  # Reuse data on next epoch\n",
    "        .shuffle(\n",
    "            buffer_size=approx_shard_train_size, seed=42, reshuffle_each_iteration=False\n",
    "        )  # Randomize shards\n",
    "        .batch(batch_size)\n",
    "        .repeat()  # Avoid last batch being of unequal size\n",
    "        .prefetch(tf.data.AUTOTUNE)  # Overlap preprocessing and training\n",
    "    )\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    model = build_model()\n",
    "    print(model.summary())\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001 * hvd.size())\n",
    "    # Horovod: add Horovod DistributedOptimizer.\n",
    "    opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "    print(\"Compiling model...\")\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=opt,\n",
    "        metrics=[\"categorical_accuracy\"],\n",
    "        experimental_run_tf_function=False,\n",
    "    )\n",
    "\n",
    "    print(\"Initializing training callbacks...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=20, verbose=0, mode=\"min\"),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.1,\n",
    "            patience=7,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=os.environ[\"TENSORBOARD_S3_ADDRESS\"],\n",
    "            histogram_freq=1,\n",
    "        ),\n",
    "        # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "        # This is necessary to ensure consistent initialization of all workers when\n",
    "        # training is started with random weights or restored from a checkpoint.\n",
    "        hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "    ]\n",
    "    # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "    if hvd.rank() == 0:\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                f\"{model_dir}/best_model.keras\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                mode=\"min\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(\"Starting model training...\")\n",
    "    start = time.time()\n",
    "    hist = model.fit(\n",
    "        distributed_train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=approx_shard_train_size // batch_size\n",
    "        + 1,  # Needed when using repeat()\n",
    "        callbacks=callbacks,\n",
    "        verbose=1 if hvd.rank() == 0 else 0,\n",
    "    )\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "        print(\"\\n\\nTraining took \", time.time() - start, \"seconds\")\n",
    "\n",
    "        print(\"Model train history:\")\n",
    "        print(hist.history)\n",
    "\n",
    "        print(f\"Saving model to: {model_dir}\")\n",
    "        model.save(model_dir)\n",
    "        print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "        print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_specification = kfp.components.func_to_component_text(\n",
    "    func=(\n",
    "        train_model\n",
    "        if int(ARGUMENTS[\"number_of_workers\"]) <= 1\n",
    "        else train_distributed_model\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Component: Evaluate model with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    test_dataset_dir: InputPath(str), model_dir: InputPath(str), batch_size: int = 20\n",
    ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    \"\"\"Loads a saved model from file and uses a pre-downloaded dataset for evaluation.\n",
    "    Model metrics are persisted to `{metrics_path}` for Kubeflow Pipelines metadata.\"\"\"\n",
    "\n",
    "    from collections import namedtuple\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "\n",
    "    test_dataset = tf.data.experimental.load(test_dataset_dir)\n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "    (loss, accuracy) = model.evaluate(test_dataset)\n",
    "\n",
    "    print((loss, accuracy))\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"loss\", \"numberValue\": str(loss), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"accuracy\", \"numberValue\": str(accuracy), \"format\": \"PERCENTAGE\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "\n",
    "    return out_tuple(json.dumps(metrics))\n",
    "\n",
    "\n",
    "evaluate_model_comp = kfp.components.create_component_from_func(\n",
    "    func=evaluate_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Create the actual pipeline by combining the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Music Genre Classification\",\n",
    "    description=\"An example pipeline that performs a music genre classification on audio data\",\n",
    ")\n",
    "def music_genre_classification_pipeline(\n",
    "    blackboard: str,\n",
    "    dataset_url: str,\n",
    "    dataset_configuration: str,\n",
    "    dataset_label_columns: List[str],\n",
    "    model_name: str,\n",
    "    cluster_configuration_secret: str,\n",
    "    training_gpus: int,\n",
    "    number_of_workers: int,\n",
    "    distribution_type: str,\n",
    "    training_node_selector: str,\n",
    "):\n",
    "    create_blackboard = dsl.VolumeOp(\n",
    "        name=\"Create Artefacts Blackboard\",\n",
    "        resource_name=blackboard,\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    load_dataset_task = CATALOG.load_dataset_comp(\n",
    "        path=dataset_url,\n",
    "        configuration=dataset_configuration,\n",
    "        label_columns=dataset_label_columns,\n",
    "    )\n",
    "    load_dataset_task.after(create_blackboard)\n",
    "\n",
    "    CATALOG.create_dataset_quality_report(\n",
    "        dataset_dir=load_dataset_task.outputs[\"dataset_dir\"],\n",
    "        dataset_type=\"huggingface\",\n",
    "        additional_args={\"split\": \"test\"},\n",
    "    )\n",
    "\n",
    "    preprocess_dataset_task = preprocess_dataset_comp(\n",
    "        dataset_dir=load_dataset_task.outputs[\"dataset_dir\"],\n",
    "    )\n",
    "\n",
    "    monitor_training_task = CATALOG.monitor_training_comp()\n",
    "\n",
    "    # InputPath and OutputPath like \"prep_dataset_dir\" & \"model_dir\":\n",
    "    # Use name of parameters of train component on right-hand side.\n",
    "    train_parameters = {\n",
    "        \"train_dataset_dir\": \"train_dataset_dir\",\n",
    "        \"validation_dataset_dir\": \"validation_dataset_dir\",\n",
    "        \"model_dir\": \"model_dir\",\n",
    "    }\n",
    "\n",
    "    distribution_specification = {\n",
    "        \"distribution_type\": distribution_type,\n",
    "        \"number_of_workers\": number_of_workers,\n",
    "    }\n",
    "\n",
    "    train_model_task = CATALOG.train_model_comp(\n",
    "        preprocess_dataset_task.outputs[\"train_dataset_dir\"],\n",
    "        preprocess_dataset_task.outputs[\"validation_dataset_dir\"],\n",
    "        train_specification,\n",
    "        train_parameters,\n",
    "        model_name=model_name,\n",
    "        gpus=training_gpus,\n",
    "        node_selector=training_node_selector,\n",
    "        tensorboard_s3_address=monitor_training_task.outputs[\"tensorboard_s3_address\"],\n",
    "        cluster_configuration_secret=cluster_configuration_secret,\n",
    "        distribution_specification=distribution_specification,\n",
    "    )\n",
    "\n",
    "    evaluate_model_comp(\n",
    "        preprocess_dataset_task.outputs[\"test_dataset_dir\"],\n",
    "        train_model_task.outputs[\"model_dir\"],\n",
    "    )\n",
    "\n",
    "    CATALOG.plot_confusion_matrix_comp(\n",
    "        input_columns=[\"mel_spectrogram\"],\n",
    "        label_columns=load_dataset_task.outputs[\"labels\"],\n",
    "        test_dataset_dir=preprocess_dataset_task.outputs[\"test_dataset_dir\"],\n",
    "        model_dir=train_model_task.outputs[\"model_dir\"],\n",
    "    )\n",
    "\n",
    "    convert_model_to_onnx_task = CATALOG.convert_model_to_onnx_comp(\n",
    "        train_model_task.outputs[\"model_dir\"]\n",
    "    )\n",
    "\n",
    "    upload_model_task = CATALOG.upload_model_comp(\n",
    "        convert_model_to_onnx_task.outputs[\"onnx_model_dir\"], project_name=model_name\n",
    "    )\n",
    "\n",
    "    deploy_model_with_kserve_task = CATALOG.deploy_model_with_kserve_comp(\n",
    "        project_name=model_name,\n",
    "        model_version=upload_model_task.outputs[\"model_version\"],\n",
    "    )\n",
    "\n",
    "    deploy_model_with_kserve_task.after(upload_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Run the pipeline within an experiment\n",
    "Create a pipeline run, using a pipeline configuration that:\n",
    "- enables data passing via persistent volumes (faster than the default MinIO-based passing)\n",
    "- disables caching (which currently is not supported for data passing via volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://www.kubeflow.org/docs/components/pipelines/overview/caching/#managing-caching-staleness\n",
    "def disable_cache_transformer(op):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf = PipelineConf()\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n",
    "pipeline_conf.data_passing_method = data_passing_methods.KubernetesVolume(\n",
    "    volume=V1Volume(\n",
    "        name=ARGUMENTS[\"blackboard\"],\n",
    "        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "            \"{{workflow.name}}-%s\" % ARGUMENTS[\"blackboard\"]\n",
    "        ),\n",
    "    ),\n",
    "    path_prefix=f'{ARGUMENTS[\"blackboard\"]}/',\n",
    ")\n",
    "\n",
    "kfp.Client().create_run_from_pipeline_func(\n",
    "    music_genre_classification_pipeline,\n",
    "    arguments=ARGUMENTS,\n",
    "    namespace=NAMESPACE,\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.) Test model deployment\n",
    "See API documentation: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md\n",
    "\n",
    "### 5.1) Check model endpoint availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = MODEL_NAME + \"-predictor-default.\" + NAMESPACE\n",
    "HEADERS = {\"Host\": HOST}\n",
    "MODEL_ENDPOINT = \"http://\" + MODEL_NAME + \"-predictor-default/v2/models/\" + MODEL_NAME\n",
    "\n",
    "res = requests.get(MODEL_ENDPOINT, headers=HEADERS)\n",
    "response = json.loads(res.text)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note you can also do this:\n",
    "```curl -H \"Host: $HOST\" $MODEL_ENDPOINT```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2) Get test audio\n",
    "See: https://commons.wikimedia.org/wiki/Category:Audio_files_of_blues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO = \"audio.ogg\"\n",
    "\n",
    "AUDIO_URL = \"https://upload.wikimedia.org/wikipedia/commons/7/7c/Boogie_lead_riff.ogg\"\n",
    "# AUDIO_URL = \"https://upload.wikimedia.org/wikipedia/commons/9/99/Blues_Rock.ogg\"\n",
    "\n",
    "!wget $AUDIO_URL -O $AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(AUDIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3) Convert test audio to 2-seconds Mel Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 22050\n",
    "N_FFT = 512\n",
    "HOP_LENGTH = N_FFT // 2\n",
    "N_MELS = 64\n",
    "\n",
    "\n",
    "def convert_to_melspecs(filename):\n",
    "    audios = get_batches(filename)\n",
    "    return batch_log_melspectrogram(audios)\n",
    "\n",
    "\n",
    "def get_batches(audio):\n",
    "    y, sr = lb.load(audio, mono=True)\n",
    "\n",
    "    duration = lb.core.get_duration(y)\n",
    "\n",
    "    audios = []\n",
    "    # prune first 2 seconds and ending (assumption: does not include important data)\n",
    "    for i in range(2, math.floor(duration), 2):\n",
    "        y_sample, sr_sample = lb.load(audio, mono=True, offset=i, duration=2.0)\n",
    "        audios.append(y_sample)\n",
    "\n",
    "    return audios\n",
    "\n",
    "\n",
    "def log_melspectrogram(data):\n",
    "    melspec = lb.feature.melspectrogram(\n",
    "        y=data, hop_length=HOP_LENGTH, n_fft=N_FFT, n_mels=N_MELS\n",
    "    )\n",
    "    return lb.power_to_db(melspec**2)\n",
    "\n",
    "\n",
    "def batch_log_melspectrogram(data_list):\n",
    "    melspecs = np.asarray(\n",
    "        [log_melspectrogram(data_list[i]) for i in range(len(data_list) - 1)]\n",
    "    )\n",
    "    melspecs = melspecs.reshape(\n",
    "        melspecs.shape[0], melspecs.shape[1], melspecs.shape[2], 1\n",
    "    )\n",
    "    return melspecs\n",
    "\n",
    "\n",
    "melspecs = convert_to_melspecs(AUDIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4) Visualize a Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle = (int)(melspecs.shape[0] / 2)\n",
    "example_melspec = melspecs[middle]\n",
    "example_melspec = example_melspec.reshape(\n",
    "    example_melspec.shape[0], example_melspec.shape[1]\n",
    ")\n",
    "\n",
    "pylab.axis(\"off\")  # no axis\n",
    "pylab.axes(\n",
    "    [0.0, 0.0, 1.0, 1.0], frameon=False, xticks=[], yticks=[]\n",
    ")  # Remove the white edge\n",
    "display.specshow(example_melspec, y_axis=\"mel\", x_axis=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5) Score example Mel Spectrogram against model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(melspec):\n",
    "    PREDICT_ENDPOINT = MODEL_ENDPOINT + \"/infer\"\n",
    "\n",
    "    payload = {\n",
    "      \"inputs\": [{\n",
    "          \"name\": \"conv2d_input\",\n",
    "          \"shape\": [1, 64, 173, 1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": melspec.tolist()\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    res = requests.post(PREDICT_ENDPOINT, headers=HEADERS, data=json.dumps(payload))\n",
    "    response = json.loads(res.text)\n",
    "    return response['outputs'][0]['data']\n",
    "\n",
    "\n",
    "test_score = score(example_melspec)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENRE_LABELS = [\n",
    "    \"Blues\",\n",
    "    \"Classical\",\n",
    "    \"Country\",\n",
    "    \"Disco\",\n",
    "    \"Hip hop\",\n",
    "    \"Jazz\",\n",
    "    \"Metal\",\n",
    "    \"Pop\",\n",
    "    \"Reggae\",\n",
    "    \"Rock\"\n",
    "]\n",
    "\n",
    "GENRE_LABELS[np.argmax(test_score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6) Score each Mel Spectrogram against deployed model & aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = [0.0 for genre in GENRE_LABELS]\n",
    "counts = [0 for genre in GENRE_LABELS]\n",
    "\n",
    "for melspec in melspecs:\n",
    "    predictions = score(melspec)\n",
    "    for i in range(len(GENRE_LABELS)):\n",
    "        probabilities[i] += predictions[i]\n",
    "    counts[np.argmax(predictions)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7) Aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(GENRE_LABELS)):\n",
    "    probabilities[i] = probabilities[i]/len(melspecs)\n",
    "    print(GENRE_LABELS[i] + \": \" + str(probabilities[i]) + \" (\" + str(counts[i]) + \"x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
