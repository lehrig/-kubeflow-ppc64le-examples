{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantically Equivalent Sentences\n",
    "\n",
    "Determine whether two sentences are sementically equivalent - from data to deployed model.\n",
    "The training data is a corpus of sentence pairs and, for each pair, a label indicating whether the sentences in the pair are semantically equivalent.\n",
    "The data stems from the \"Microsoft Research Paraphrase Corpus\", which was extracted from online news sources and subsequently labeled by humans.\n",
    "\n",
    "\n",
    "This notebook was adapted from the quick start guide of ðŸ¤— Datasets [2].\n",
    "\n",
    "## Author\n",
    "- Sebastian Lehrig <sebastian.lehrig1@ibm.com>\n",
    "\n",
    "## License\n",
    "Apache-2.0 License\n",
    "\n",
    "## References\n",
    "[1] Original data: https://www.microsoft.com/en-us/download/details.aspx?id=52398\n",
    "\n",
    "[2] Original code snippets from ðŸ¤— Datasets: https://huggingface.co/docs/datasets/quickstart#nlp\n",
    "\n",
    "[3] ðŸ¤— Dataset: https://huggingface.co/datasets/glue/viewer/mrpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.) Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "from kubernetes.client.models import V1Volume, V1PersistentVolumeClaimVolumeSource\n",
    "import os\n",
    "from pydoc import importfile\n",
    "import requests\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment-specific configurations\n",
    "# - Activate train-bursting by setting a CLUSTER_CONFIGURATION_SECRET containing the remote cluster configuration\n",
    "# - Activate distributed training by setting NUMBER_OF_WORKERS > 1; TRAINING_GPUS hold per worker\n",
    "#\n",
    "# %env CLUSTER_CONFIGURATION_SECRET remote-power-cluster\n",
    "# %env CLUSTER_CONFIGURATION_SECRET remote-x86-cluster\n",
    "# %env CLUSTER_CONFIGURATION_SECRET remote-x86-telekom-cluster\n",
    "# %env TRAINING_GPUS 0\n",
    "# %env NUMBER_OF_WORKERS 2\n",
    "# %env TRAINING_NODE_SELECTOR nvidia.com/gpu.product: \"Tesla-V100-SXM2-32GB\"\n",
    "# %env TRAINING_NODE_SELECTOR kubernetes.io/hostname: node2\n",
    "# %env TRAINING_NODE_SELECTOR worker_type: baremetal_worker\n",
    "#\n",
    "# Reset:\n",
    "# del os.environ['CLUSTER_CONFIGURATION_SECRET']\n",
    "# del os.environ['TRAINING_GPUS']\n",
    "# del os.environ['TRAINING_NODE_SELECTOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\"\n",
    "COMPONENT_CATALOG_GIT = \"https://github.com/lehrig/kubeflow-ppc64le-components.git\"\n",
    "COMPONENT_CATALOG_RELEASE = \"main\"\n",
    "\n",
    "NUMBER_OF_WORKER = os.getenv(\"NUMBER_OF_WORKERS\", default=\"1\")\n",
    "\n",
    "ARGUMENTS = {\n",
    "    \"blackboard\": \"artefacts\",\n",
    "    \"dataset_url\": \"glue\",\n",
    "    \"dataset_configuration\": \"mrpc\",\n",
    "    \"dataset_label_columns\": [\"label\"],\n",
    "    \"model_name\": \"equal-sentences\",\n",
    "    \"cluster_configuration_secret\": os.getenv(\n",
    "        \"CLUSTER_CONFIGURATION_SECRET\", default=\"\"\n",
    "    ),\n",
    "    \"training_gpus\": os.getenv(\"TRAINING_GPUS\", default=\"0\"),\n",
    "    \"number_of_workers\": NUMBER_OF_WORKER,\n",
    "    \"distribution_type\": \"Job\" if int(NUMBER_OF_WORKER) <= 1 else \"MPI\",\n",
    "    \"training_node_selector\": os.getenv(\"TRAINING_NODE_SELECTOR\", default=\"\"),\n",
    "}\n",
    "MODEL_NAME = ARGUMENTS[\"model_name\"]\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "    NAMESPACE = f.read()\n",
    "\n",
    "ARGUMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Load catalog with reusable Kubeflow components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --branch $COMPONENT_CATALOG_RELEASE $COMPONENT_CATALOG_GIT $COMPONENT_CATALOG_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = importfile(f\"{COMPONENT_CATALOG_FOLDER}/catalog.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Create custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Component: Preprocess data (tokenize etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(\n",
    "    dataset_dir: InputPath(str),\n",
    "    train_dataset_dir: OutputPath(str),\n",
    "    validation_dataset_dir: OutputPath(str),\n",
    "    test_dataset_dir: OutputPath(str),\n",
    "    batch_size: int = 200,\n",
    "):\n",
    "    \"\"\"Tokenizes dataset and preprares it to be processed with BertForSequenceClassification. Saves result into `prep_dataset_dir`.\"\"\"\n",
    "\n",
    "    from datasets import load_from_disk\n",
    "    import os\n",
    "    from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "    print(f\"Loading input dataset from {dataset_dir}...\")\n",
    "    dataset = load_from_disk(dataset_dir)\n",
    "\n",
    "    print(\"Printing first training dataset entry (unprocessed):\")\n",
    "    print(dataset[\"train\"][0])\n",
    "\n",
    "    # load a pretrained BERT model and its corresponding tokenizer from the\n",
    "    # Huggingface Transformers library.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "    # tokenize the dataset & truncate and pad the text into tidy rectangular\n",
    "    # tensors. The tokenizer generates three new columns in the dataset:\n",
    "    # input_ids, token_type_ids, and an attention_mask.\n",
    "    # These are the model inputs.\n",
    "    def encode(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"sentence1\"],\n",
    "            examples[\"sentence2\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    prep_dataset = dataset.map(\n",
    "        encode, batched=True, batch_size=batch_size, num_proc=2, keep_in_memory=True\n",
    "    )\n",
    "\n",
    "    print(\"Printing first training dataset entry (tokenized):\")\n",
    "    print(prep_dataset[\"train\"][0])\n",
    "\n",
    "    # Rename the label column to labels, which is the expected input name in\n",
    "    # BertForSequenceClassification:\n",
    "    prep_dataset = prep_dataset.map(\n",
    "        lambda examples: {\"labels\": examples[\"label\"]},\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        num_proc=2,\n",
    "        keep_in_memory=True,\n",
    "    )\n",
    "\n",
    "    def save_as_tfdataset(\n",
    "        dataset, columns, label_columns, data_collator, directory, shuffle\n",
    "    ):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        tf_dataset = dataset.to_tf_dataset(\n",
    "            columns=columns,\n",
    "            label_cols=label_columns,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving pre-processed dataset to '{directory}'...\")\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        tf.data.Dataset.save(tf_dataset, directory)\n",
    "\n",
    "        print(f\"Pre-processed dataset saved. Contents of '{directory}':\")\n",
    "        print(os.listdir(directory))\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "    columns = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "    label_columns = [\"labels\"]\n",
    "    save_as_tfdataset(\n",
    "        prep_dataset[\"train\"],\n",
    "        columns,\n",
    "        label_columns,\n",
    "        data_collator,\n",
    "        train_dataset_dir,\n",
    "        True,\n",
    "    )\n",
    "    save_as_tfdataset(\n",
    "        prep_dataset[\"validation\"],\n",
    "        columns,\n",
    "        label_columns,\n",
    "        data_collator,\n",
    "        validation_dataset_dir,\n",
    "        False,\n",
    "    )\n",
    "    save_as_tfdataset(\n",
    "        prep_dataset[\"test\"],\n",
    "        columns,\n",
    "        label_columns,\n",
    "        data_collator,\n",
    "        test_dataset_dir,\n",
    "        False,\n",
    "    )\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "\n",
    "preprocess_dataset_comp = kfp.components.create_component_from_func(\n",
    "    func=preprocess_dataset, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Specification: Train the model (used as parameter to train component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_dataset_dir: InputPath(str),\n",
    "    validation_dataset_dir: InputPath(str),\n",
    "    model_dir: OutputPath(str),\n",
    "    epochs: int = 4,\n",
    "    batch_size: int = 2,\n",
    "):\n",
    "    \"\"\"Trains CNN model. Once trained, the model is persisted to `model_dir`.\"\"\"\n",
    "\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.callbacks import (\n",
    "        EarlyStopping,\n",
    "        ModelCheckpoint,\n",
    "        ReduceLROnPlateau,\n",
    "        TensorBoard,\n",
    "    )\n",
    "    import time\n",
    "    from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "    def load_datasets():\n",
    "        train_dataset = tf.data.Dataset.load(train_dataset_dir)\n",
    "        validation_dataset = tf.data.Dataset.load(validation_dataset_dir)\n",
    "        return (train_dataset, validation_dataset)\n",
    "\n",
    "    def build_model():\n",
    "        return TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset, validation_dataset = load_datasets()\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    model = build_model()\n",
    "    print(model.summary())\n",
    "\n",
    "    print(\"Compiling model...\")\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, from_logits=True\n",
    "        ),\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    print(\"Initializing training callbacks...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=20, verbose=0, mode=\"min\"),\n",
    "        ModelCheckpoint(\n",
    "            f\"{model_dir}/best_model.keras\",\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.1,\n",
    "            patience=7,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=os.environ[\"TENSORBOARD_S3_ADDRESS\"],\n",
    "            histogram_freq=1,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    print(\"Starting model training...\")\n",
    "    start = time.time()\n",
    "    hist = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=100,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    print(\"\\n\\nTraining took \", time.time() - start, \"seconds\")\n",
    "\n",
    "    print(\"Model train history:\")\n",
    "    print(hist.history)\n",
    "\n",
    "    print(f\"Saving model to: {model_dir}\")\n",
    "    model.save(model_dir)\n",
    "    print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "    print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Training: Un-collapse below cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_distributed_model(\n",
    "    train_dataset_dir: InputPath(str),\n",
    "    validation_dataset_dir: InputPath(str),\n",
    "    model_dir: OutputPath(str),\n",
    "    epochs: int = 4,\n",
    "    batch_size: int = 2,\n",
    "):\n",
    "    \"\"\"Trains CNN model. Once trained, the model is persisted to `model_dir`.\"\"\"\n",
    "\n",
    "    import horovod.tensorflow.keras as hvd\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.callbacks import (\n",
    "        EarlyStopping,\n",
    "        ModelCheckpoint,\n",
    "        ReduceLROnPlateau,\n",
    "        TensorBoard,\n",
    "    )\n",
    "    import time\n",
    "    import tensorflow_datasets as tfds\n",
    "    from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "    def load_datasets():\n",
    "        train_dataset = tf.data.Dataset.load(train_dataset_dir)\n",
    "        validation_dataset = tf.data.Dataset.load(validation_dataset_dir)\n",
    "        return (train_dataset, validation_dataset)\n",
    "\n",
    "    def build_model():\n",
    "        return TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "    print(\"Initializing Horovod/MPI for distributed training...\")\n",
    "    hvd.init()\n",
    "\n",
    "    # Pin GPU to be used to process local rank (one GPU per process)\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], \"GPU\")\n",
    "\n",
    "    # Prepare distributed training with GPU support\n",
    "    os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "    tfds.disable_progress_bar()\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "    # see https://horovod.readthedocs.io/en/stable/api.html\n",
    "    print(\"==============================================\")\n",
    "    print(f\"hvd.rank(): {str(hvd.rank())}\")\n",
    "    print(f\"hvd.local_rank(): {str(hvd.local_rank())}\")\n",
    "    print(f\"hvd.size(): {str(hvd.size())}\")\n",
    "    print(f\"hvd.local_size(): {str(hvd.local_size())}\")\n",
    "    print(\"gpus:\")\n",
    "    print(gpus)\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset, validation_dataset = load_datasets()\n",
    "\n",
    "    print(\"Making traininig dataset ready for distributed training...\")\n",
    "    # Best shuffling needs a buffer with size equal to the size of the\n",
    "    # dataset. Approximate values should be fine here.\n",
    "    dataset_elements = 4076  # hard to determine dynamically in TFDataset\n",
    "    approx_shard_train_size = dataset_elements // hvd.size() + 1\n",
    "\n",
    "    # References:\n",
    "    # - shard: https://github.com/horovod/horovod/issues/2623#issuecomment-768435610\n",
    "    # - cache & prefetch: https://stackoverflow.com/questions/59228816/what-do-the-tensorflow-datasets-functions-cache-and-prefetch-do\n",
    "    # - shuffle: https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling\n",
    "    distributed_train_dataset = (\n",
    "        train_dataset.unbatch()  # Batch after sharding\n",
    "        .shard(num_shards=hvd.size(), index=hvd.rank())  # 1 shard per worker\n",
    "        .cache()  # Reuse data on next epoch\n",
    "        .shuffle(\n",
    "            buffer_size=approx_shard_train_size, seed=42, reshuffle_each_iteration=False\n",
    "        )  # Randomize shards\n",
    "        .batch(batch_size)\n",
    "        .repeat()  # Avoid last batch being of unequal size\n",
    "        .prefetch(tf.data.AUTOTUNE)  # Overlap preprocessing and training\n",
    "    )\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    model = build_model()\n",
    "    print(model.summary())\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001 * hvd.size())\n",
    "    # Horovod: add Horovod DistributedOptimizer.\n",
    "    opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "    print(\"Compiling model...\")\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, from_logits=True\n",
    "        ),\n",
    "        optimizer=opt,\n",
    "        metrics=[\"accuracy\"],\n",
    "        experimental_run_tf_function=False,\n",
    "    )\n",
    "\n",
    "    print(\"Initializing training callbacks...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=20, verbose=0, mode=\"min\"),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.1,\n",
    "            patience=7,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=os.environ[\"TENSORBOARD_S3_ADDRESS\"],\n",
    "            histogram_freq=1,\n",
    "        ),\n",
    "        # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "        # This is necessary to ensure consistent initialization of all workers when\n",
    "        # training is started with random weights or restored from a checkpoint.\n",
    "        hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "    ]\n",
    "    # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "    if hvd.rank() == 0:\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                f\"{model_dir}/best_model.keras\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                mode=\"min\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(\"Starting model training...\")\n",
    "    start = time.time()\n",
    "    hist = model.fit(\n",
    "        distributed_train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=approx_shard_train_size // batch_size\n",
    "        + 1,  # Needed when using repeat()\n",
    "        callbacks=callbacks,\n",
    "        verbose=1 if hvd.rank() == 0 else 0,\n",
    "    )\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "        print(\"\\n\\nTraining took \", time.time() - start, \"seconds\")\n",
    "\n",
    "        print(\"Model train history:\")\n",
    "        print(hist.history)\n",
    "\n",
    "        print(f\"Saving model to: {model_dir}\")\n",
    "        model.save(model_dir)\n",
    "        print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "        print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_specification = kfp.components.func_to_component_text(\n",
    "    func=(\n",
    "        train_model\n",
    "        if int(ARGUMENTS[\"number_of_workers\"]) <= 1\n",
    "        else train_distributed_model\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Component: Evaluate model with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    test_dataset_dir: InputPath(str), model_dir: InputPath(str), batch_size: int = 20\n",
    ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    \"\"\"Loads a saved model from file and uses a pre-downloaded dataset for evaluation.\n",
    "    Model metrics are persisted to `{metrics_path}` for Kubeflow Pipelines metadata.\"\"\"\n",
    "\n",
    "    from collections import namedtuple\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "\n",
    "    test_dataset = tf.data.experimental.load(test_dataset_dir)\n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "    (loss, accuracy) = model.evaluate(test_dataset)\n",
    "\n",
    "    print((loss, accuracy))\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"loss\", \"numberValue\": str(loss), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"accuracy\", \"numberValue\": str(accuracy), \"format\": \"PERCENTAGE\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "\n",
    "    return out_tuple(json.dumps(metrics))\n",
    "\n",
    "\n",
    "evaluate_model_comp = kfp.components.create_component_from_func(\n",
    "    func=evaluate_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Create the actual pipeline by combining the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Semantically Equal Sentences\",\n",
    "    description=\"An example pipeline that performs a an NLP task (determining whether two sentences are semantically equivalent)\",\n",
    ")\n",
    "def semantic_equivalence_pipeline(\n",
    "    blackboard: str,\n",
    "    dataset_url: str,\n",
    "    dataset_configuration: str,\n",
    "    dataset_label_columns: List[str],\n",
    "    model_name: str,\n",
    "    cluster_configuration_secret: str,\n",
    "    training_gpus: int,\n",
    "    number_of_workers: int,\n",
    "    distribution_type: str,\n",
    "    training_node_selector: str,\n",
    "):\n",
    "    create_blackboard = dsl.VolumeOp(\n",
    "        name=\"Create Artefacts Blackboard\",\n",
    "        resource_name=blackboard,\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    load_dataset_task = CATALOG.load_dataset_comp(\n",
    "        path=dataset_url,\n",
    "        configuration=dataset_configuration,\n",
    "        label_columns=dataset_label_columns,\n",
    "    )\n",
    "    load_dataset_task.after(create_blackboard)\n",
    "\n",
    "    CATALOG.create_dataset_quality_report(\n",
    "        dataset_dir=load_dataset_task.outputs[\"dataset_dir\"],\n",
    "        dataset_type=\"huggingface\",\n",
    "        additional_args={\"split\": \"test\"},\n",
    "    )\n",
    "\n",
    "    preprocess_dataset_task = preprocess_dataset_comp(\n",
    "        dataset_dir=load_dataset_task.outputs[\"dataset_dir\"],\n",
    "    )\n",
    "\n",
    "    monitor_training_task = CATALOG.monitor_training_comp()\n",
    "\n",
    "    # InputPath and OutputPath like \"prep_dataset_dir\" & \"model_dir\":\n",
    "    # Use name of parameters of train component on right-hand side.\n",
    "    train_parameters = {\n",
    "        \"train_dataset_dir\": \"train_dataset_dir\",\n",
    "        \"validation_dataset_dir\": \"validation_dataset_dir\",\n",
    "        \"model_dir\": \"model_dir\",\n",
    "    }\n",
    "\n",
    "    distribution_specification = {\n",
    "        \"distribution_type\": distribution_type,\n",
    "        \"number_of_workers\": number_of_workers,\n",
    "    }\n",
    "\n",
    "    train_model_task = CATALOG.train_model_comp(\n",
    "        preprocess_dataset_task.outputs[\"train_dataset_dir\"],\n",
    "        preprocess_dataset_task.outputs[\"validation_dataset_dir\"],\n",
    "        train_specification,\n",
    "        train_parameters,\n",
    "        model_name=model_name,\n",
    "        gpus=training_gpus,\n",
    "        node_selector=training_node_selector,\n",
    "        tensorboard_s3_address=monitor_training_task.outputs[\"tensorboard_s3_address\"],\n",
    "        cluster_configuration_secret=cluster_configuration_secret,\n",
    "        distribution_specification=distribution_specification,\n",
    "    )\n",
    "\n",
    "    evaluate_model_comp(\n",
    "        preprocess_dataset_task.outputs[\"test_dataset_dir\"],\n",
    "        train_model_task.outputs[\"model_dir\"],\n",
    "    )\n",
    "\n",
    "    CATALOG.plot_confusion_matrix_comp(\n",
    "        input_columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "        label_columns=load_dataset_task.outputs[\"labels\"],\n",
    "        test_dataset_dir=preprocess_dataset_task.outputs[\"test_dataset_dir\"],\n",
    "        model_dir=train_model_task.outputs[\"model_dir\"],\n",
    "    )\n",
    "\n",
    "    convert_model_to_onnx_task = CATALOG.convert_model_to_onnx_comp(\n",
    "        train_model_task.outputs[\"model_dir\"]\n",
    "    )\n",
    "\n",
    "    upload_model_task = CATALOG.upload_model_comp(\n",
    "        convert_model_to_onnx_task.outputs[\"onnx_model_dir\"], project_name=model_name\n",
    "    )\n",
    "\n",
    "    deploy_model_with_kserve_task = CATALOG.deploy_model_with_kserve_comp(\n",
    "        project_name=model_name,\n",
    "        model_version=upload_model_task.outputs[\"model_version\"],\n",
    "    )\n",
    "\n",
    "    deploy_model_with_kserve_task.after(upload_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Run the pipeline within an experiment\n",
    "Create a pipeline run, using a pipeline configuration that:\n",
    "- enables data passing via persistent volumes (faster than the default MinIO-based passing)\n",
    "- disables caching (which currently is not supported for data passing via volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://www.kubeflow.org/docs/components/pipelines/overview/caching/#managing-caching-staleness\n",
    "def disable_cache_transformer(op):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf = PipelineConf()\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n",
    "pipeline_conf.data_passing_method = data_passing_methods.KubernetesVolume(\n",
    "    volume=V1Volume(\n",
    "        name=ARGUMENTS[\"blackboard\"],\n",
    "        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "            \"{{workflow.name}}-%s\" % ARGUMENTS[\"blackboard\"]\n",
    "        ),\n",
    "    ),\n",
    "    path_prefix=f'{ARGUMENTS[\"blackboard\"]}/',\n",
    ")\n",
    "\n",
    "kfp.Client().create_run_from_pipeline_func(\n",
    "    semantic_equivalence_pipeline,\n",
    "    arguments=ARGUMENTS,\n",
    "    namespace=NAMESPACE,\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.) Test model deployment\n",
    "See API documentation: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md\n",
    "\n",
    "### 5.1) Check model endpoint availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = MODEL_NAME + \"-predictor-default.\" + NAMESPACE\n",
    "HEADERS = {'Host': HOST}\n",
    "MODEL_ENDPOINT = \"http://\" + MODEL_NAME + \"-predictor-default/v2/models/\" + MODEL_NAME\n",
    "\n",
    "res = requests.get(MODEL_ENDPOINT, headers=HEADERS)\n",
    "response = json.loads(res.text)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note you can also do this:\n",
    "```curl -H \"Host: $HOST\" $MODEL_ENDPOINT```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2) Get example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE1 = \"Kubeflow on IBM Power rocks!\"\n",
    "SENTENCE2 = \"Empowering the most efficient & dependable MLOps platforms in the world via Kubeflow on Power!\"\n",
    "\n",
    "example = {\"sentence1\": SENTENCE1, \"sentence2\": SENTENCE2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3) Encode example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def encode(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence1\"],\n",
    "        example[\"sentence2\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "\n",
    "encoded_example = encode(example)\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(encoded_example.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3) Score example sentences against model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_ENDPOINT = MODEL_ENDPOINT + \"/infer\"\n",
    "\n",
    "payload = {\n",
    "  \"inputs\": [{\n",
    "      \"name\": \"input_ids/input_ids\",\n",
    "      \"datatype\": \"INT32\",\n",
    "      \"shape\": [1, 5],\n",
    "      \"data\": encoded_example\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "res = requests.post(PREDICT_ENDPOINT, headers=HEADERS, data=json.dumps(payload))\n",
    "response = json.loads(res.text)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
